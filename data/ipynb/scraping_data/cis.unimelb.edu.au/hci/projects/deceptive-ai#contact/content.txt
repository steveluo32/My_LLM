URL: https://cis.unimelb.edu.au/hci/projects/deceptive-ai#contact





Deceptive AI: Human-Computer Interaction, The University of Melbourne













































        Skip to main content
      





Deceptive AIProjectsHuman-Computer InteractionSchool of Computing and Information Systems


School of Computing and Information Systems



Human-Computer Interaction



Projects



Deceptive AI





Search

Menu










Go







Close







Deceptive AI

On this pageProject overviewProject teamPublicationsProject informationContact details
Project overviewCan computers deceive people? It is clear that computers can be used as tools for people to deceive each other (eg, fake news, phishing, etc), but is it possible for a specially designed AI agent to engage in strategic deception? In other words, can a machine devise and enact deeply deceptive strategies against humans by reasoning about their perceptions, beliefs and intentions? In what kind of human–machine encounters might this be possible? What would be the nature of the machine’s computational and cognitive architecture? How do people understand the possibilities of such machine deception and how do they react to it?We are a team of computer scientists, psychologists, and magicians who are collaborating to explore these questions. Our methodology is to formalise the techniques of deception used by stage conjurors (for example see Kuhn, Olson & Raz, 2016) such that they can be built into the thinking processes of software agents, and to test the deceptive powers of these agents when playing computer games against humans (See Smith, Dignum & Sonenberg, 2016). The project sheds light on what it means for a computer to intentionally deceive people, and provides insights into the capabilities of software agents to deploy advanced ‘theory-of-mind’ reasoning in human-machine encounters.Project teamWally Smith, Faculty of Engineering & Information Technology, The University of MelbourneLiz Sonenberg, Faculty of Engineering & Information Technology, The University of MelbourneMichael Kirley, Faculty of Engineering & Information Technology, The University of MelbourneFrank Dignum, Department of Computing Science, Umeå University, SwedenGustav Kuhn, Department of Psychology, Goldsmiths, University of LondonPeta Masters, Faculty of Engineering & Information Technology, The University of MelbournePublicationsSmith, W., Dignum, F. & Sonenberg, L. (2016). The construction of impossibility: a logic-based analysis of conjuring tricks. Frontiers in psychology, 7, 748. [View abstract]Psychologists and cognitive scientists have long drawn insights and evidence from stage magic about human perceptual and attentional errors. We present a complementary analysis of conjuring tricks that seeks to understand the experience of impossibility that they produce. Our account is first motivated by insights about the constructional aspects of conjuring drawn from magicians’ instructional texts. A view is then presented of the logical nature of impossibility as an unresolvable contradiction between a perception-supported belief about a situation and a memory-supported expectation. We argue that this condition of impossibility is constructed not simply through misperceptions and misattentions, but rather it is an outcome of a trick’s whole structure of events. This structure is conceptualised as two parallel event sequences: an effect sequence that the spectator is intended to believe; and a method sequence that the magician understands as happening. We illustrate the value of this approach through an analysis of a simple close-up trick, Martin Gardner’s Turnabout. A formalism called propositional dynamic logic is used to describe some of its logical aspects. This elucidates the nature and importance of the relationship between a trick’s effect sequence and its method sequence, characterised by the careful arrangement of four evidence relationships: similarity, perceptual equivalence, structural equivalence, and congruence. The analysis further identifies two characteristics of magical apparatus that enable the construction of apparent impossibility: substitutable elements and stable occlusion.CloseMasters, P., Smith, W., Sonenberg, L. & Kirley, M. (2021) ‘Characterising Deception in AI: A Survey’. In Sarkadi, Wright, Masters, McBurnely (Eds). Deceptive AI. Springer. (pp. 17 – 26)Smith, W., Kirley, M., Sonenberg, L. & Dignum F. (2021) ‘The role of environments in affording deceptive behaviour: some preliminary insights from stage magic’. In Sarkadi, Wright, Masters, McBurnely (Eds). Deceptive AI. Springer.Masters, P. & Vered, M., (2021, August). What’s the context? Implicit and Explicit Assumptions in Model-Based Goal Recognition. In Proceedings of the 30th International Joint Conferences on Artificial Intelligence (to appear). [View abstract]Every model involves assumptions. While some are standard to all models that simulate intelligent decision-making (eg, discrete/continuous, static/dynamic), goal recognition is well known also to involve choices about the observed agent: is it aware of being observed? cooperative or adversarial? In this paper, we examine not only these but the many other assumptions made in the context of model-based goal recognition. By exploring their meaning, the relationships between them and the confusions that can arise, we demonstrate their importance, shed light on the way trends emerge in AI, and suggest a novel means for researchers to uncover suitable avenues for future work.CloseSmith, W. (2021). ‘Deceptive Strategies in the Miniature Illusions of Close-Up Magic’. In Rein, K. (Ed.) Illusion in Cultural Practice: Productive Deception. Routledge. (pp. 123 – 138)Masters, P., Kirley, M., & Smith, W. (2021, May). Extended Goal Recognition: A Planning-Based Model for Strategic Deception. In Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems (pp 871–879). [View abstract]Goal recognition is the problem of determining an agent’s intent by observing its actions. In the context of AI research, the problem is tackled for two quite different purposes: to determine an agent’s most probable goal or, for human-aware planning including planned—or strategic—deception, to determine an observer’s most likely belief about that goal. Making no distinction, contemporary models tend to assume an infallible observer, deceived only while it has limited access to information or if the environment itself is only partially observable. Focusing on the second purpose, we propose an extended framework that incorporates formal definitions of confirmation bias, selective attention and memory decay. In contrast to pre-existing models, our approach combines explicit consideration of prior probabilities with a principled representation of observer confidence and distinguishes between potential observations, ie, every observable event within the observer’s frame of reference—and recalled observations which we model as a function of attention and memory. We show that when these factors are taken into consideration, false beliefs may arise and can be made to persist, even in a fully observable environment—thus providing a perceptual model readily incorporated into the ‘thinking’ of an adversarial agent for the purpose of strategic deception.CloseMasters, P., Smith, W., & Kirley, M. (2021). Extended Goal Recognition: Lessons from Magic. Frontiers in artificial intelligence, 4, 730990.Liu, Z., Yang, Y., Miller, T., & Masters, P. (2021, May). Deceptive Reinforcement Learning for Privacy-Preserving Planning. In Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems (pp. 818–826). [View abstract]In this paper, we study the problem of deceptive reinforcement learning to preserve the privacy of a reward function. Reinforcement learning is the problem of finding a behaviour policy based on rewards received from exploratory behaviour. A key ingredient in reinforcement learning is a reward function, which determines how much reward (negative or positive) is given and when. However, in some situations, we may want to keep a reward function private; that is, to make it difficult for an observer to determine the reward function used. We define the problem of privacy-preserving reinforcement learning, and present two models for solving it. These models are based on dissimulation – a form of deception that ‘hides the truth’. We evaluate our models both computationally and via human behavioural experiments. Results show that the resulting policies are indeed deceptive, and that participants can determine the true reward function less reliably than that of an honest agentCloseMasters, P., & Sardina, S. (2021). Expecting the unexpected: Goal recognition for rational and irrational agents. Artificial Intelligence, 297, 103490. [View abstract]Contemporary cost-based goal-recognition assumes rationality: that observed behaviour is more or less optimal. Probabilistic goal recognition systems, however, explicitly depend on some degree of sub-optimality to generate probability distributions. We show that, even when an observed agent is only slightly irrational (sub-optimal), state-of-the-art systems produce counter-intuitive results (though these may only become noticeable when the agent is highly irrational). We provide a definition of rationality appropriate to situations where the ground truth is unknown, define a rationality measure (RM) that quantifies an agent’s expected degree of sub-optimality, and define an innovative self-modulating probability distribution formula for goal recognition. Our formula recognises sub-optimality and adjusts its level of confidence accordingly, thereby handling irrationality—and rationality—in an intuitive, principled manner. Building on that formula, moreover, we strengthen a previously published result, showing that “single-observation” recognition in the path-planning domain achieves identical results to more computationally expensive techniques, where previously we claimed only to achieve equivalent rankings though values differed.CloseProject information Funding source
         ARC Grant DP180101215 ‘A Computational Theory of Strategic Deception’ 
       Project time frame
         2018–2020 
      Contact detailsAssoc Prof Wally Smith

Email: wsmith@unimelb.edu.au








Email Contact us
Faculty of Engineering and Information Technology
Engineering & IT Students


Staff only: FEIT Intranet



      Site footer   We acknowledge and pay respect to the Traditional Owners of the lands upon which our campuses are situated   Read about our commitment to reconciliation        About us     Careers at Melbourne     Safety and respect     Newsroom     Contact        Phone: 13 MELB (13 6352)  International: +61 3 9035 5511   Address:  The University of Melbourne  Grattan Street, Parkville  Victoria, 3010, Australia   View all Campus locations                  Facebook 

 Share on Facebook      LinkedIn 

 Share on LinkedIn      Instagram 

 Share on Instagram      Twitter 

 Share on Twitter                 Emergency     Terms & privacy     Accessibility     Privacy       The University of Melbourne  (Australian University): PRV12150    CRICOS number: 00116K   ABN: 84 002 705 224      

CloseSchool of Computing and Information Systems
About usAbout us
About the SchoolAbout the School
Life in Melbourne
Alumni profiles
Casual tutor, demonstrator, marker and project team supervisor opportunities
CSIRAC: Our first computerCSIRAC: Our first computer
CSIRAC’s vital statistics
CSIRAC chronology
CSIRAC design
How did CSIRAC work?: Storage
How did CSIRAC work?: Console
CSIRAC uses
CSIRAC: Designers
The music of CSIRAC
CSIRAC emulator
Jurij Semkiw
CSIRAC and computer history links
The Last of the First, CSIRAC: Australia’s First Computer
CSIRAC photo gallery
The history of computing at the University of MelbourneThe history of computing at the University of Melbourne
History of computing in the department
Starting the Department of Information Systems
Early internet
Memories of the department
Programming
Student life in the department
Women in computing
CIS Doctoral ColloquiumCIS Doctoral Colloquium
2024 CIS Doctoral Colloquium2024 CIS Doctoral Colloquium
Submission Guidelines
2023 CIS Doctoral Colloquium2023 CIS Doctoral Colloquium
Submission Guidelines
Colloquium Photos
2022 CIS Doctoral Colloquium2022 CIS Doctoral Colloquium
Submission Guidelines
Program_open
Participation and Awards
Volunteers and Judges
CISDC 2022 Photos
2019 CIS Doctoral Colloquium
2018 CIS Doctoral Colloquium
2017 CIS Doctoral Colloquium
2016 CIS Doctoral Colloquium
2015 CIS Doctoral Colloquium
2014 CIS Doctoral Colloquium
2013 CIS Doctoral Colloquium
Keynote speaker
Colloquium sponsors
Committee
Call for papers
Application_closed
Submission_info
Program_pending
Program_open
Alumni
Registration_pending
Engage with the School of Computing and Information SystemsEngage with the School of Computing and Information Systems
Research collaborationResearch collaboration
Cyber attack maps to underpin better strategic responses
Surgeons gain implant expertise with virtual training
Informatics analyses value in digital health technologies
Data contrasting highlights changing use of city
Online community designed to support mental health for young people
Satisfaction score to improve quality of internet search results
Supply chain scheduling keeps automated mining operations on task
Combined data adds power to decision-making
New algorithms help interpret vision loss from digital images
Host a student intern
Student industry projects
Become a guest speaker
Cremorne Digital Hub
NewsNews
2022 news and events2022 news and events
CIS-EEE Seed Funding
How data can prevent overdiagnosis

CIS - IE 2022 Research Collaboration Seed Funding
2021 news and events
2020 news and events
2019 news and events
2023 news and events2023 news and events
CIS-ME 2023 Seed Funding round
CIS-IE 2023 Seed Funding round
People
ResearchResearch
Artificial intelligenceArtificial intelligence
Graduate Reseachers
Computer scienceComputer science
Graduate Researchers
Digital Health
Research projects
Seminars
Schools
Study with usStudy with us
Undergraduate programs
Graduate coursework programs
Graduate research programs
Industry based learning
Programming proficiency test
First Year CentreFirst Year Centre
About
Study with CIS
Undergraduate programs
Get help
People
Become a tutor
Contact
Graduate programsGraduate programs
Graduate coursework programs
Graduate research programs
AI and Autonomy LabAI and Autonomy Lab
News
People
Publications
ResearchResearch
Mining and optimisation
Automated planning languages
Foundations of human-agent collaboration
People-oriented software engineering
Explainable artificial intelligence
Multimodal human-agent collaboration
Artificial Intelligence Assurance LabArtificial Intelligence Assurance Lab
About us
Research
Publications
People
Industry engagement
Academic Centre of Cyber Security Excellence (ACCSE)
Human-Computer InteractionHuman-Computer Interaction
FacilitiesFacilities
User Experience Lab
Interactive Technologies Lab
Engineering Workshop

Telstra Creator Space

News and Events
PeoplePeople
Staff
Graduate researchers
Alumni
ProjectsProjects
Adaptive learning technologies
Ageing and avatars
AI-enabled assistance for strategic planning in games
Wearable technology for arm monitoring in health
Augmented fitness
Augmented learning environment for physiotherapy education
Biometric Mirror
Changing views
Citizen Heritage
Cognition-aware systems
Cognitive interaction
Completed projects
Connecting learners for collaboration across diverse communities
Cross-community information systems
Crowdsourcing
Death and the Internet
Deceptive AI
Designing for scale
Designing technologies for indigenous knowledge
Digital commemoration
Digital domesticity
Conceptualising and measuring digital emotion regulation
Emerging technologies for enrichment in old age
Encounters
Ethics and digital games
Evaluation of natural user interfaces in query auto-completion
Examining the ‘digital’ in hybrid digital boardgames
Exploring complex data sets using highly engaging environments
Exploring natural user interfaces during meal times
FaceSpace
Getting well and being present
Growing old and staying connected
HandLog: tangible interactions for game input and rehabilitation
iFISH
Improving Vitamin D status and related health in young women
Insertable technology for human interactions
Interactive displays
Interactive spaces and media architecture
Kinecting with orang-utans
Mediating intimacy
Mobile fieldwork and learning
Multimodal human–agent collaboration
Music streaming and algorithmic recommendation
Near-infrared spectroscopy
Onebody
Orygen Virtual World Project
Promoting student peer review in Australian tertiary education
Personal sensing for mental health and wellbeing
Pholiota Unlocked
Reading on ubiquitous devices
Smart Garden Watering
Smartphones for science
Social gaming events: Warhammer 40K
Social networking sites for ambivalent socialisers
Social play in immersive gaming environments
Social robots and virtual assistants for older people
Sociophysical interactions
Social Orientated Requirements Engineering
Spectating eSports and Let’s Play
SpinalLog
Supporting social interactions for video calls in the home
Teleconsultation: enhancing interactions between clinicians and patients
Virtual co-presence
Virtual Reality and climate change communication
VR therapy for youth mental health
Robot Assisted Learning and Rehabilitation
XR for Human-Robot Interaction
Social and Domestic Drones 
Human-Centred Agent Learning
Child of Now
 #thismymob
Publications
ResearchResearch
Smart Spaces
Ubiquitous computing
Digital health
Digital nature
Novel interactions
Design for ageing
Games and play
Social computing and communities
Human-Robot Interaction
SeminarsSeminars
Past seminars 2020
Past seminars 2019
Past seminars 2016–2018
Past seminars 2011–2015
Past seminars 2006–2010
Past seminars 2004–2005
StudyStudy
HCI programs for potential students
HCI subjects
Masters projects
Potential PhD students
Information systemsInformation systems
Process science and technology
Business analytics and decision making
Cybersecurity management
Digital health
Innovations in the digital society
Publications
Contact us
Current Students
Library
Staff



,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,






    This site uses and shares cookies and similar technologies to personalise your experience, advertise to you and provide content from third-parties as well as analyse our usage. You consent to our use of such technologies by proceeding. You can change your mind or consent choices at any time. Visit our Privacy Statement  for further information.
           Accept cookies       Cookie preferences              
====================================================================================================

