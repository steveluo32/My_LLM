{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15c969d8-eaa7-4be7-8df0-02dd986b7389",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import random\n",
    "# import time\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# from urllib.parse import urljoin\n",
    "\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "# from selenium.webdriver.common.by import By\n",
    "# import time\n",
    "\n",
    "# def crawl_site(start_url, visited_urls_file, urls_file, not_visiting_urls_file):\n",
    "    \n",
    "#     # 初始化Options\n",
    "#     options = Options()\n",
    "#     options.add_argument('--headless')  # 无头模式\n",
    "#     options.add_argument('--disable-gpu')\n",
    "#     options.add_argument('--window-size=1920x1080')  # 设置窗口大小，这对某些页面布局很重要\n",
    "#     options.add_argument('--disable-extensions')\n",
    "#     options.add_argument('--disable-dev-shm-usage')  # Overcome limited resource problems\n",
    "#     options.add_argument('--no-sandbox')  # Bypass OS security model\n",
    "\n",
    "#     # 初始化Chrome WebDriver\n",
    "#     driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    \n",
    "#     # 初始化集合\n",
    "#     not_visiting_urls = set()\n",
    "#     visited_urls = set()\n",
    "#     urls_to_visit = set([start_url])\n",
    "\n",
    "#     # 在函数的开头打开文件，并在整个函数执行期间保持打开状态\n",
    "#     with open(urls_file, 'a+', encoding='utf-8') as urls_f, \\\n",
    "#         open(visited_urls_file, 'a+', encoding='utf-8') as visited_urls_f, \\\n",
    "#         open(not_visiting_urls_file, 'r', encoding='utf-8') as not_visiting_urls_f:\n",
    "#         # 读取 urls_file 文件并更新 urls_to_visit 集合\n",
    "#         urls_f.seek(0)\n",
    "#         for url in urls_f:\n",
    "#             urls_to_visit.add(url.strip())\n",
    "\n",
    "#         # 读取 visited_urls_file 文件并更新 visited_urls 集合\n",
    "#         visited_urls_f.seek(0)\n",
    "#         for url in visited_urls_f:\n",
    "#             visited_urls.add(url.strip())\n",
    "            \n",
    "#         not_visiting_urls_f.seek(0)\n",
    "#         for url in not_visiting_urls_f:\n",
    "#             not_visiting_urls.add(url.strip())\n",
    "\n",
    "#         # 从 urls_to_visit 中去除已访问过的 URLs\n",
    "#         urls_to_visit.difference_update(visited_urls)\n",
    "\n",
    "#         while urls_to_visit:\n",
    "#             current_url = urls_to_visit.pop()\n",
    "\n",
    "#             # 只有当current_url不在visited_urls里时才进行爬取\n",
    "#             if current_url not in visited_urls and all(sub_url not in current_url for sub_url in not_visiting_urls):\n",
    "#                 print(f\"Visiting: {current_url}\")\n",
    "#                 # 在尝试访问链接前，添加随机延迟\n",
    "#                 time.sleep(random.randint(1, 3))\n",
    "\n",
    "#                 try:\n",
    "#                     # response = requests.get(current_url)\n",
    "#                     # soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#                     driver.get(current_url)\n",
    "#                     soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                    \n",
    "\n",
    "#                     # 提取并存储所有文字内容\n",
    "#                     text_content = soup.get_text()\n",
    "\n",
    "#                     # 移除current_url中的\"https:\"，并为文件路径准备\n",
    "#                     sanitized_url = current_url.replace('https://', '').replace('http://', '')\n",
    "#                     content_directory = f\"scraping_data/{sanitized_url}/\"\n",
    "#                     content_file_path = os.path.join(content_directory, \"content.txt\") #加个scraping_data\n",
    "                    \n",
    "#                     # 找到所有的动态加载的div元素\n",
    "#                     dynamic_divs = driver.find_elements(By.CSS_SELECTOR, 'div.js-card-gallery')\n",
    "                    \n",
    "#                     # 确保目录存在\n",
    "#                     os.makedirs(content_directory, exist_ok=True)\n",
    "\n",
    "#                     with open(content_file_path, 'w', encoding='utf-8') as content_f:\n",
    "#                         content_f.write(f\"URL: {current_url}\\n{text_content}\\n{'='*100}\\n\\n\")\n",
    "\n",
    "#                         # 遍历每个div并写入它的文本内容\n",
    "#                         for div in dynamic_divs:\n",
    "#                             content_f.write(f\"{div.text}\\n\")  # 写入div的文本内容\n",
    "                    \n",
    "#                     if \"cis\" in current_url:\n",
    "                            \n",
    "#                         # 查找并处理所有链接\n",
    "#                         for link in soup.find_all('a', href=True):\n",
    "#                             absolute_link = urljoin(current_url, link['href'])\n",
    "#                             if (absolute_link not in visited_urls) and \\\n",
    "#                             (absolute_link not in urls_to_visit) and \\\n",
    "#                             all(sub_url not in absolute_link for sub_url in not_visiting_urls):\n",
    "#                                 urls_to_visit.add(absolute_link)\n",
    "#                                 urls_f.write(f\"{absolute_link}\\n\")\n",
    "                    \n",
    "#                         for div in dynamic_divs:\n",
    "#                             # 在每个div中查找所有的<a>标签\n",
    "#                             links = div.find_elements(By.TAG_NAME, 'a')\n",
    "#                             # 输出每个链接的href属性\n",
    "#                             for link in links:\n",
    "#                                 div_url = link.get_attribute('href')\n",
    "#                                 if (absolute_link not in visited_urls) and \\\n",
    "#                                 (absolute_link not in urls_to_visit) and \\\n",
    "#                                 all(sub_url not in absolute_link for sub_url in not_visiting_urls):\n",
    "#                                     urls_to_visit.add(div_url)\n",
    "#                                     urls_f.write(f\"{div_url}\\n\")\n",
    "\n",
    "\n",
    "#                         visited_urls.add(current_url)\n",
    "#                         visited_urls_f.write(f\"{current_url}\\n\")\n",
    "#                 except requests.RequestException as e:\n",
    "#                     print(f\"Error during requests to {current_url}: {str(e)}\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     start_url = 'https://cis.unimelb.edu.au/'\n",
    "#     visited_urls_file = 'scraping_urls/visited_urls.txt'\n",
    "#     urls_file = 'scraping_urls/urls.txt'\n",
    "#     not_visiting_urls_file = 'scraping_urls/not_visiting_urls.txt'\n",
    "#     crawl_site(start_url, visited_urls_file, urls_file, not_visiting_urls_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75f4a426-a214-4227-819d-e88f8694ce38",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "def crawl_site(start_url, visited_urls_file, urls_file, not_visiting_urls_file):\n",
    "    \n",
    "    # 初始化Options\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')  # 无头模式\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--window-size=1920x1080')  # 设置窗口大小，这对某些页面布局很重要\n",
    "    options.add_argument('--disable-extensions')\n",
    "    options.add_argument('--disable-dev-shm-usage')  # Overcome limited resource problems\n",
    "    options.add_argument('--no-sandbox')  # Bypass OS security model\n",
    "    \n",
    "    # 设置模拟用户的请求头\n",
    "    user_agent = (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/104.0.5112.79 Safari/537.36\"\n",
    "    )\n",
    "    options.add_argument(f'user-agent={user_agent}')\n",
    "    options.add_argument('accept-language=en-US,en;q=0.9')\n",
    "    options.add_argument('accept-encoding=gzip, deflate, br')\n",
    "    options.add_argument('referer=https://www.google.com')\n",
    "    \n",
    "    # 初始化Chrome WebDriver\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    \n",
    "    # 初始化集合\n",
    "    not_visiting_urls = set()\n",
    "    visited_urls = set()\n",
    "    urls_to_visit = set([start_url])\n",
    "\n",
    "    # 在函数的开头打开文件，并在整个函数执行期间保持打开状态\n",
    "    with open(urls_file, 'a+', encoding='utf-8') as urls_f, \\\n",
    "        open(visited_urls_file, 'a+', encoding='utf-8') as visited_urls_f, \\\n",
    "        open(not_visiting_urls_file, 'r', encoding='utf-8') as not_visiting_urls_f:\n",
    "        # 读取 urls_file 文件并更新 urls_to_visit 集合\n",
    "        urls_f.seek(0)\n",
    "        for url in urls_f:\n",
    "            urls_to_visit.add(url.strip())\n",
    "\n",
    "        # 读取 visited_urls_file 文件并更新 visited_urls 集合\n",
    "        visited_urls_f.seek(0)\n",
    "        for url in visited_urls_f:\n",
    "            visited_urls.add(url.strip())\n",
    "            \n",
    "        not_visiting_urls_f.seek(0)\n",
    "        for url in not_visiting_urls_f:\n",
    "            not_visiting_urls.add(url.strip())\n",
    "\n",
    "        # 从 urls_to_visit 中去除已访问过的 URLs\n",
    "        urls_to_visit.difference_update(visited_urls)\n",
    "\n",
    "        while urls_to_visit:\n",
    "            current_url = urls_to_visit.pop()\n",
    "\n",
    "            # 只有当current_url不在visited_urls里时才进行爬取\n",
    "            if current_url not in visited_urls and all(sub_url not in current_url for sub_url in not_visiting_urls) \\\n",
    "                and \"unimelb\" in current_url \\\n",
    "                and \"linkedin\" not in current_url:\n",
    "                print(f\"Visiting: {current_url}\")\n",
    "                # 在尝试访问链接前，添加随机延迟\n",
    "                time.sleep(random.randint(1, 3))\n",
    "\n",
    "                try:\n",
    "                    # 访问当前URL并解析页面内容\n",
    "                    driver.get(current_url)\n",
    "                    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "                    # 提取并存储所有文字内容\n",
    "                    text_content = soup.get_text()\n",
    "\n",
    "                    # 移除current_url中的\"https:\"，并为文件路径准备\n",
    "                    sanitized_url = current_url.replace('https://', '').replace('http://', '')\n",
    "                    content_directory = f\"scraping_data/{sanitized_url}/\"\n",
    "                    content_file_path = os.path.join(content_directory, \"content.txt\") #加个scraping_data\n",
    "                    \n",
    "                    # 找到所有的动态加载的div元素\n",
    "                    dynamic_divs = driver.find_elements(By.CSS_SELECTOR, 'div.js-card-gallery')\n",
    "                    \n",
    "                    # 确保目录存在\n",
    "                    os.makedirs(content_directory, exist_ok=True)\n",
    "\n",
    "                    with open(content_file_path, 'w', encoding='utf-8') as content_f:\n",
    "                        content_f.write(f\"URL: {current_url}\\n{text_content}\\n{'='*100}\\n\\n\")\n",
    "\n",
    "                        # 遍历每个div并写入它的文本内容\n",
    "                        for div in dynamic_divs:\n",
    "                            content_f.write(f\"{div.text}\\n\")  # 写入div的文本内容\n",
    "                    \n",
    "                    if \"cis\" in current_url:\n",
    "                            \n",
    "                        # 查找并处理所有链接\n",
    "                        for link in soup.find_all('a', href=True):\n",
    "                            absolute_link = urljoin(current_url, link['href'])\n",
    "                            if (absolute_link not in visited_urls) and \\\n",
    "                            (absolute_link not in urls_to_visit) and \\\n",
    "                            (\"unimelb\" in absolute_link) and \\\n",
    "                            all(sub_url not in absolute_link for sub_url in not_visiting_urls):\n",
    "                                urls_to_visit.add(absolute_link)\n",
    "                                urls_f.write(f\"{absolute_link}\\n\")\n",
    "                    \n",
    "                        for div in dynamic_divs:\n",
    "                            # 在每个div中查找所有的<a>标签\n",
    "                            links = div.find_elements(By.TAG_NAME, 'a')\n",
    "                            # 输出每个链接的href属性\n",
    "                            for link in links:\n",
    "                                div_url = link.get_attribute('href')\n",
    "                                if (div_url not in visited_urls) and \\\n",
    "                                (div_url not in urls_to_visit) and \\\n",
    "                                (\"unimelb\" in div_url) and \\\n",
    "                                all(sub_url not in div_url for sub_url in not_visiting_urls):\n",
    "                                    urls_to_visit.add(div_url)\n",
    "                                    urls_f.write(f\"{div_url}\\n\")\n",
    "\n",
    "\n",
    "                    visited_urls.add(current_url)\n",
    "                    visited_urls_f.write(f\"{current_url}\\n\")\n",
    "                except WebDriverException as e:\n",
    "                    # Catch specific errors like ERR_NAME_NOT_RESOLVED and move on to the next URL\n",
    "                    if \"ERR_NAME_NOT_RESOLVED\" in str(e):\n",
    "                        print(f\"Skipping URL due to error: {current_url} - {str(e)}\")\n",
    "                        continue\n",
    "                    else:\n",
    "                        print(f\"Error during requests to {current_url}: {str(e)}\")\n",
    "                        continue\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_url = 'https://cis.unimelb.edu.au/'\n",
    "    visited_urls_file = 'scraping_urls/visited_urls.txt'\n",
    "    urls_file = 'scraping_urls/urls.txt'\n",
    "    not_visiting_urls_file = 'scraping_urls/not_visiting_urls.txt'\n",
    "    crawl_site(start_url, visited_urls_file, urls_file, not_visiting_urls_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
